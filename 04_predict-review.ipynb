{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffda6baa",
   "metadata": {},
   "source": [
    "### Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3afb4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab07391",
   "metadata": {},
   "source": [
    "### Load pre-trained objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba2737ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")            # Fitted TF-IDF vectorizer\n",
    "kmeans_final = joblib.load(\"kmeans_model.pkl\")              # Fitted MiniBatchKMeans model\n",
    "threshold = joblib.load(\"anomaly_distance_threshold.pkl\")   # Load threshold\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e764f2f1",
   "metadata": {},
   "source": [
    "### Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57c63189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review_text):\n",
    "    review_text_cleaned = review_text.lower()\n",
    "    review_text_cleaned = re.sub(r'[^a-z\\s]', '', review_text_cleaned)\n",
    "\n",
    "    tokens = []\n",
    "    for word in review_text_cleaned.split():\n",
    "        if word not in stop_words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')  # Using verb as POS\n",
    "            tokens.append(lemma)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569aa3f8",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7db24fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review(review_text):\n",
    "    # Step 1: Preprocess the review\n",
    "    processed_text = preprocess_review(review_text)\n",
    "    \n",
    "    # Step 2: Transform into TF-IDF features\n",
    "    tfidf_features = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Step 3: Add review length feature\n",
    "    review_length = len(processed_text.split())\n",
    "    length_feature = csr_matrix([[review_length]])\n",
    "    \n",
    "    # Step 4: Combine features\n",
    "    final_features = hstack([tfidf_features, length_feature])\n",
    "    \n",
    "    # Step 5: Predict cluster and calculate distance to centroid\n",
    "    cluster_label = kmeans_final.predict(final_features)[0]\n",
    "    centroid = kmeans_final.cluster_centers_[cluster_label]\n",
    "    \n",
    "    # Calculate Euclidean distance\n",
    "    distance = euclidean(final_features.toarray().ravel(), centroid)\n",
    "    \n",
    "    # Step 6: Load and apply anomaly threshold\n",
    "    is_anomalous = distance > threshold\n",
    "    review_type = 'Anomalous' if is_anomalous else 'Normal'\n",
    "    \n",
    "    return review_type, cluster_label, distance, processed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f35b0c",
   "metadata": {},
   "source": [
    "### Test with sample reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7a2e88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Prediction Results:\n",
      "================================================================================\n",
      "\n",
      "Review 1:\n",
      "Original: Love this!  Well made, sturdy, and very comfortable.  I love it!Very pretty\n",
      "Processed: love well make sturdy comfortable love itvery pretty\n",
      "Cluster: 0, Distance: 11.9406\n",
      "Prediction: Normal (?)\n",
      "----------------------------------------\n",
      "\n",
      "Review 2:\n",
      "Original: love it, a great upgrade from the original.  I've had mine for a couple of years\n",
      "Processed: love great upgrade original ive mine couple years\n",
      "Cluster: 0, Distance: 11.9410\n",
      "Prediction: Normal (?)\n",
      "----------------------------------------\n",
      "\n",
      "Review 3:\n",
      "Original: Panget\n",
      "Processed: panget\n",
      "Cluster: 0, Distance: 18.8998\n",
      "Prediction: Normal (?)\n",
      "----------------------------------------\n",
      "\n",
      "Review 4:\n",
      "Original: Not impossible to put together by yourself. Only scratched one place in a not very noticeable place. Get many compliments on it and has lots of storage.\n",
      "Processed: impossible put together scratch one place noticeable place get many compliment lot storage\n",
      "Cluster: 0, Distance: 6.9711\n",
      "Prediction: Normal (?)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test with both normal and anomalous examples\n",
    "    test_reviews = [\n",
    "        \"Love this!  Well made, sturdy, and very comfortable.  I love it!Very pretty\",\n",
    "        \"love it, a great upgrade from the original.  I've had mine for a couple of years\",\n",
    "        \"Panget\",\n",
    "        \"Not impossible to put together by yourself. Only scratched one place in a not very noticeable place. Get many compliments on it and has lots of storage.\",\n",
    "    ]\n",
    "    \n",
    "    print(\"Review Prediction Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, review in enumerate(test_reviews, 1):\n",
    "        prediction, cluster, distance, processed = predict_review(review)\n",
    "        \n",
    "        print(f\"\\nReview {i}:\")\n",
    "        print(f\"Original: {review}\")\n",
    "        print(f\"Processed: {processed}\")\n",
    "        print(f\"Cluster: {cluster}, Distance: {distance:.4f}\")\n",
    "        print(f\"Prediction: {prediction} ({'âœ“' if 'Anomalous' in prediction and 'terrible' in review.lower() or 'horrible' in review.lower() else '?'})\")\n",
    "        print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
